{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine-Learning-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1hBGiGEhztNCeA6ZgMRSW7MatIATHNYnO",
      "authorship_tag": "ABX9TyMxf5dERiU+hDsmm6KFHc73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aura-byte/machine-learning/blob/main/Machine_Learning_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b5RL-tl3F46"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "!unzip gdrive/MyDrive/Project-Data/test1.zip\n",
        "!unzip gdrive/MyDrive/Project-Data/train.zip\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# organize dataset into a useful structure\n",
        "from os import makedirs\n",
        "from os import listdir\n",
        "from shutil import copyfile\n",
        "from random import seed\n",
        "from random import random\n",
        "# create directories\n",
        "dataset_home = 'dataset_dogs_vs_cats/'\n",
        "subdirs = ['train/', 'test/']\n",
        "for subdir in subdirs:\n",
        "    # create label subdirectories\n",
        "    labeldirs = ['dogs/', 'cats/']\n",
        "    for labldir in labeldirs:\n",
        "        newdir = dataset_home + subdir + labldir\n",
        "        makedirs(newdir, exist_ok=True)\n",
        "# seed random number generator\n",
        "seed(1)\n",
        "# define ratio of pictures to use for validation\n",
        "val_ratio = 0.25\n",
        "# copy training dataset images into subdirectories\n",
        "src_directory = 'train/'\n",
        "for file in listdir(src_directory):\n",
        "    src = src_directory + '/' + file\n",
        "    dst_dir = 'train/'\n",
        "    if random() < val_ratio:\n",
        "        dst_dir = 'test/'\n",
        "    if file.startswith('cat'):\n",
        "        dst = dataset_home + dst_dir + 'cats/'  + file\n",
        "        copyfile(src, dst)\n",
        "    elif file.startswith('dog'):\n",
        "        dst = dataset_home + dst_dir + 'dogs/'  + file\n",
        "        copyfile(src, dst)"
      ],
      "metadata": {
        "id": "9PuT77IhTV8G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ", mkdir\n",
        "import tensorflow\n",
        "import sys\n",
        "from matplotlib import pyplot \n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "\n",
        "\n",
        "DATASET_LOCATION = \"dataset_dogs_vs_cats/\"\n",
        "CATS_OUTPUT = 0.0\n",
        "DOGS_OUTPUT = 1.0\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"Function will create a model and will return it.\"\"\"\n",
        "    model = Sequential([\n",
        "        layers.Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        \n",
        "    opt = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    model = create_model()\n",
        "    # create data generator - scaling the pixels between 0 and 1\n",
        "    datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
        "    # prepare iterators\n",
        "    train_it = datagen.flow_from_directory(DATASET_LOCATION + 'train/', class_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "    test_it = datagen.flow_from_directory(DATASET_LOCATION + 'test/', class_mode='binary', batch_size=64, target_size=(200, 200))\n",
        "    #Save the weights using the `checkpoint_path` format\n",
        "    model.save_weights(\"Model_Checkpoints/\".format(epoch=0))\n",
        "    #fit the model:\n",
        "    history = model.fit(\n",
        "        train_it,\n",
        "        steps_per_epoch=len(train_it),\n",
        "        validation_data=test_it,\n",
        "        validation_steps=len(test_it), \n",
        "        callbacks=[\"Model_Checkpoints/\"],\n",
        "        epochs=20)\n",
        "    # evaluate model\n",
        "    _, acc = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
        "    print('> %.3f' % (acc * 100.0))\n",
        "    summarize_diagnostics(history)\n",
        "    model.save('saved_model/my_model')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "S5HJ0YvwRQKb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}